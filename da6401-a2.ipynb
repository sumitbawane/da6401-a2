{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T06:02:55.536276Z",
     "iopub.status.busy": "2025-04-19T06:02:55.536064Z",
     "iopub.status.idle": "2025-04-19T06:02:59.433692Z",
     "shell.execute_reply": "2025-04-19T06:02:59.432937Z",
     "shell.execute_reply.started": "2025-04-19T06:02:55.536250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:44:36.169206Z",
     "iopub.status.busy": "2025-04-19T07:44:36.168583Z",
     "iopub.status.idle": "2025-04-19T07:44:36.173125Z",
     "shell.execute_reply": "2025-04-19T07:44:36.172407Z",
     "shell.execute_reply.started": "2025-04-19T07:44:36.169180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/inaturalist/inaturalist_12K/train'\n",
    "data_path_test='/kaggle/input/inaturalist/inaturalist_12K/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:03:05.731309Z",
     "iopub.status.busy": "2025-04-19T06:03:05.731070Z",
     "iopub.status.idle": "2025-04-19T06:03:14.089808Z",
     "shell.execute_reply": "2025-04-19T06:03:14.089229Z",
     "shell.execute_reply.started": "2025-04-19T06:03:05.731293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m049\u001b[0m (\u001b[33mcs24m049-iit-m\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='9f6e625b6a4825fa64c9ba29384c657072eb3b12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:03:16.806282Z",
     "iopub.status.busy": "2025-04-19T06:03:16.805899Z",
     "iopub.status.idle": "2025-04-19T06:03:16.820051Z",
     "shell.execute_reply": "2025-04-19T06:03:16.819290Z",
     "shell.execute_reply.started": "2025-04-19T06:03:16.806261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "#from PP_data import preprocess_data, show_images\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_channels=3, kernel_size=[], no_kernels=[], fc1_size=512, conv_activation='ReLU', use_batch_norm=True,dropout=0.5):\n",
    "        \n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_activation = conv_activation\n",
    "        self.fc1_size = fc1_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.no_kernels = no_kernels\n",
    "        self.input_channels = input_channels\n",
    "        self.use_batch_norm = use_batch_norm  \n",
    "        self.dropout=dropout  # Dropout probability\n",
    "        # Flag to enable/disable Batch Normalization\n",
    "        # Define convolutional layers with optional Batch Normalization\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(input_channels, no_kernels[0], kernel_size=kernel_size[0], stride=1, padding=kernel_size[0] // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(no_kernels[0]) if use_batch_norm else None  # Optional Batch Norm\n",
    "        self.conv2 = nn.Conv2d(no_kernels[0], no_kernels[1], kernel_size=kernel_size[1], stride=1, padding=kernel_size[1] // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(no_kernels[1]) if use_batch_norm else None\n",
    "        self.conv3 = nn.Conv2d(no_kernels[1], no_kernels[2], kernel_size=kernel_size[2], stride=1, padding=kernel_size[2] // 2)\n",
    "        self.bn3 = nn.BatchNorm2d(no_kernels[2]) if use_batch_norm else None\n",
    "        self.conv4 = nn.Conv2d(no_kernels[2], no_kernels[3], kernel_size=kernel_size[3], stride=1, padding=kernel_size[3] // 2)\n",
    "        self.bn4 = nn.BatchNorm2d(no_kernels[3]) if use_batch_norm else None\n",
    "        self.conv5 = nn.Conv2d(no_kernels[3], no_kernels[4], kernel_size=kernel_size[4], stride=1, padding=kernel_size[4] // 2)\n",
    "        self.bn5 = nn.BatchNorm2d(no_kernels[4]) if use_batch_norm else None\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(no_kernels[4] * 7 * 7, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, num_classes)\n",
    "        \n",
    "        self.dropout_layer=nn.Dropout(p=self.dropout) if self.dropout>0 else None# Optional Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.apply_batch_norm(self.conv1(x), self.bn1))  # Apply Batch Norm if enabled\n",
    "        x = self.pool(self.apply_batch_norm(self.conv2(x), self.bn2))\n",
    "        x = self.pool(self.apply_batch_norm(self.conv3(x), self.bn3))\n",
    "        x = self.pool(self.apply_batch_norm(self.conv4(x), self.bn4))\n",
    "        x = self.pool(self.apply_batch_norm(self.conv5(x), self.bn5))\n",
    "        x = x.view(-1, self.no_kernels[4] * 7 * 7)  # Flatten the tensor\n",
    "        x = self.activation(self.fc1(x))\n",
    "        if self.dropout_layer is not None:\n",
    "            x=self.dropout_layer(x)  # Apply Dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def apply_batch_norm(self, x, bn_layer):\n",
    "        if self.use_batch_norm and bn_layer is not None:\n",
    "            return self.activation(bn_layer(x))  # Apply Batch Norm and activation\n",
    "        else:\n",
    "            return self.activation(x)  # Skip Batch Norm and apply activation\n",
    "\n",
    "    def activation(self, x):\n",
    "        if self.conv_activation == 'ReLU':\n",
    "            return F.relu(x)\n",
    "        elif self.conv_activation == 'Sigmoid':\n",
    "            return F.sigmoid(x)\n",
    "        elif self.conv_activation == 'Tanh':\n",
    "            return F.tanh(x)\n",
    "        elif self.conv_activation == 'GELU':\n",
    "            return F.gelu(x)\n",
    "        elif self.conv_activation == 'SiLU':\n",
    "            return F.silu(x)\n",
    "        elif self.conv_activation == 'Mish':\n",
    "            return F.mish(x)\n",
    "        elif self.conv_activation == 'ELU':\n",
    "            return F.elu(x)\n",
    "        elif self.conv_activation == 'SELU':\n",
    "            return F.selu(x)\n",
    "        elif self.conv_activation == 'LeakyReLU':\n",
    "            return F.leaky_relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:31:19.393059Z",
     "iopub.status.busy": "2025-04-18T19:31:19.392781Z",
     "iopub.status.idle": "2025-04-18T19:33:39.224141Z",
     "shell.execute_reply": "2025-04-18T19:33:39.220479Z",
     "shell.execute_reply.started": "2025-04-18T19:31:19.393039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: abnvkgmf\n",
      "Sweep URL: https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/sweeps/abnvkgmf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dx5no7bq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: GELU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbase_filter: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfc_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_strategy: halving\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_sizes: [5, 5, 7, 7, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007713339180941757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_type: Nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_193129-dx5no7bq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/runs/dx5no7bq' target=\"_blank\">vital-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/sweeps/abnvkgmf' target=\"_blank\">https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/sweeps/abnvkgmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20' target=\"_blank\">https://wandb.ai/cs24m049-iit-m/da6401%20a2%20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/sweeps/abnvkgmf' target=\"_blank\">https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/sweeps/abnvkgmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/runs/dx5no7bq' target=\"_blank\">https://wandb.ai/cs24m049-iit-m/da6401%20a2%20/runs/dx5no7bq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "#from cnn import SimpleCNN\n",
    "import torch.amp as amp\n",
    "import os\n",
    "wandb._service_wait = 60\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    config = wandb.config  # Access sweep parameters\n",
    "    \n",
    "    # Set device (use GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    scaler = amp.GradScaler(enabled=device.type == 'cuda')\n",
    "    \n",
    "    # Set a meaningful name for the run\n",
    "    wandb.run.name = (\n",
    "    f\"base_filter_size_{config.base_filter}_\"\n",
    "    f\"filter_strategy_{config.filter_strategy}_\"\n",
    "    f\"kernel_{'_'.join(map(str, config.kernel_sizes))}_\"\n",
    "    f\"activation_{config.activation}_batchnorm_{config.batch_norm}_\"\n",
    "    f\"dropout_{config.dropout}_fcsize_{config.fc_size}_epochs_{config.epochs}_\"\n",
    "    f\"augmentation_{config.data_augmentation}_lr_{config.learning_rate:.1e}_\"\n",
    "    f\"batchsize_{config.batch_size}\"\n",
    "    )\n",
    "    base_filter = config.base_filter  # Base filter size for the first layer\n",
    "    filters=[]\n",
    "    if config['filter_strategy'] == 'same':\n",
    "        filters = [base_filter] * 5\n",
    "    elif config['filter_strategy'] == 'doubling':\n",
    "       filters = [base_filter * (2 ** i) for i in range(5)]\n",
    "    elif config['filter_strategy'] == 'halving':\n",
    "        filters = [base_filter * (2 ** i) for i in reversed(range(5))]\n",
    "\n",
    "    # Extract parameters from wandb config\n",
    "    no_kernels =filters\n",
    "    kernel_size = config.kernel_sizes\n",
    "    activation = config.activation\n",
    "    batch_norm = config.batch_norm\n",
    "    dropout = config.dropout\n",
    "    fc_size = config.fc_size\n",
    "    learning_rate = config.learning_rate\n",
    "    batch_size = config.batch_size\n",
    "    data_augmentation = config.data_augmentation\n",
    "    epochs = config.epochs\n",
    "    val_split = 0.2  # 20% of data for validation\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SimpleCNN(\n",
    "        num_classes=10,\n",
    "        kernel_size=kernel_size,\n",
    "        no_kernels=no_kernels,\n",
    "        fc1_size=fc_size,\n",
    "        conv_activation=activation,\n",
    "        use_batch_norm=batch_norm,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    #model=nn.DataParallel(model)  # Use DataParallel for multi-GPU training if available\n",
    "    model=model.to(device)  # Move model to the appropriate device\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.optimizer_type == 'Adam':\n",
    "        optimizer =optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    elif config.optimizer_type == 'Nadam':\n",
    "        optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Load and split the dataset into training and validation sets\n",
    "    dataset = datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=get_transforms(data_augmentation, is_training=True)\n",
    "    )\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Update validation set transforms (no augmentation for validation)\n",
    "    val_dataset.dataset.transform = get_transforms(False, is_training=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        pin_memory=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # No need to shuffle validation data\n",
    "        pin_memory=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = run_epoch(model, train_loader, criterion, optimizer, device,scaler, is_training=True)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            val_loss, val_accuracy = run_epoch(model, val_loader, criterion, optimizer, device,scaler,is_training=False)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), f\"best_model_{wandb.run.id}.pth\")\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_accuracy\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Log final best validation accuracy and hyperparameters\n",
    "    wandb.log({\n",
    "        'best_val_acc': best_val_accuracy,\n",
    "        'batch_size': batch_size,\n",
    "        'activation': config.activation,\n",
    "        'batch_norm': config.batch_norm,\n",
    "        'dropout': config.dropout,\n",
    "        'fc_size': config.fc_size,\n",
    "        'learning_rate': config.learning_rate,\n",
    "        'data_augmentation': config.data_augmentation,\n",
    "        'epochs': config.epochs,\n",
    "        'base_filter': config.base_filter,\n",
    "        'filter_strategy': config.filter_strategy,\n",
    "        'kernel_sizes': config.kernel_sizes\n",
    "    })\n",
    "    \n",
    "    # Update the sweep metric\n",
    "    wandb.run.summary[\"val_acc\"] = best_val_accuracy\n",
    "\n",
    "\n",
    "def run_epoch(model, dataloader, criterion, optimizer, device, scaler,is_training=True):\n",
    "    \"\"\"Run one epoch of training or validation.\"\"\"\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "         # Forward pass with autocast for mixed precision\n",
    "        with amp.autocast(device_type=device.type,enabled= scaler.is_enabled() and is_training):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization (only during training)\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            # Use scaler for mixed precision gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def get_transforms(data_augmentation, is_training=True):\n",
    "    \"\"\"Get the appropriate transforms based on whether we're training and using augmentation.\"\"\"\n",
    "    if is_training and data_augmentation:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Optimization method: 'grid', 'random', or 'bayes'\n",
    "    'metric': {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        # strategy for filter size\n",
    "        'filter_strategy': {'values': ['same', 'doubling', 'halving']},\n",
    "        #base filter size\n",
    "        'base_filter': {'values': [32,64]},\n",
    "        \n",
    "        # Kernel sizes for each layer\n",
    "        'kernel_sizes': {'values': [[3, 3, 5, 5, 7], [5, 5, 7, 7, 3], [3, 5, 7, 5, 3],[3,3,3,3,3],[5,5,5,5,5]]},\n",
    "        'optimizer_type': {'values': ['Adam', 'Nadam']},\n",
    "        # Activation function\n",
    "        'activation': {'values': ['ReLU', 'GELU', 'SiLU', 'Mish']},\n",
    "\n",
    "        # Batch normalization\n",
    "        'batch_norm': {'values': [True, False]},\n",
    "\n",
    "        # Dropout\n",
    "        'dropout': {'values': [0.0, 0.2, 0.3]},\n",
    "\n",
    "        # Fully connected layer size\n",
    "        'fc_size': {'values': [256, 512]},\n",
    "\n",
    "        # Learning rate\n",
    "        'learning_rate': {\"distribution\": \"log_uniform_values\", \"min\": 1e-5, \"max\": 1e-3},\n",
    "\n",
    "        # Batch size\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "\n",
    "        # Data augmentation\n",
    "        'data_augmentation': {'values': [True, False]},\n",
    "\n",
    "        # Number of epochs\n",
    "        'epochs': {'values': [10,15,20]}\n",
    "    }\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"da6401 a2\")\n",
    "    wandb.agent(sweep_id, train, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:44:48.911902Z",
     "iopub.status.busy": "2025-04-19T07:44:48.911147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n",
      "100%|██████████| 82.7M/82.7M [00:00<00:00, 107MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc=51.60%, Val Acc=69.53%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# Init W&B\n",
    "#wandb.init(project=\"finetune_inaturalist\", name=\"efficientnetv2_finetune\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transforms\n",
    "def get_transforms(is_train=True):\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(data_path, transform=get_transforms(is_train=True))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "val_ds.dataset.transform = get_transforms(is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load EfficientNetV2 pre-trained\n",
    "model = models.efficientnet_v2_s(weights=\"EfficientNet_V2_S_Weights.DEFAULT\")\n",
    "\n",
    "# Modify final layer\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, 10)\n",
    "\n",
    "# Freeze all layers first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Strategy: unfreeze last N layers\n",
    "N = 20\n",
    "for param in list(model.parameters())[-N:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# Training + Validation loop\n",
    "def run_epoch(model, dataloader, train=False):\n",
    "    model.train() if train else model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    return running_loss / total, 100 * correct / total\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, train=False)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_efficientnetv2.pth\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "#wandb.run.summary[\"best_val_acc\"] = best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:04:22.274785Z",
     "iopub.status.busy": "2025-04-19T06:04:22.273975Z",
     "iopub.status.idle": "2025-04-19T06:18:37.317605Z",
     "shell.execute_reply": "2025-04-19T06:18:37.316823Z",
     "shell.execute_reply.started": "2025-04-19T06:04:22.274760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training with best configuration...\n",
      "New best model saved with validation accuracy: 29.56%\n",
      "Epoch [1/15], Train Loss: 2.0945, Train Accuracy: 25.73%, Val Loss: 1.9876, Val Accuracy: 29.56%\n",
      "New best model saved with validation accuracy: 32.72%\n",
      "Epoch [2/15], Train Loss: 1.9206, Train Accuracy: 32.55%, Val Loss: 1.8898, Val Accuracy: 32.72%\n",
      "New best model saved with validation accuracy: 34.72%\n",
      "Epoch [3/15], Train Loss: 1.8234, Train Accuracy: 35.94%, Val Loss: 1.8348, Val Accuracy: 34.72%\n",
      "Epoch [4/15], Train Loss: 1.7435, Train Accuracy: 38.69%, Val Loss: 1.8599, Val Accuracy: 34.67%\n",
      "New best model saved with validation accuracy: 36.02%\n",
      "Epoch [5/15], Train Loss: 1.6676, Train Accuracy: 41.73%, Val Loss: 1.8514, Val Accuracy: 36.02%\n",
      "New best model saved with validation accuracy: 37.57%\n",
      "Epoch [6/15], Train Loss: 1.5788, Train Accuracy: 44.84%, Val Loss: 1.7799, Val Accuracy: 37.57%\n",
      "Epoch [7/15], Train Loss: 1.4928, Train Accuracy: 47.76%, Val Loss: 1.8051, Val Accuracy: 37.22%\n",
      "New best model saved with validation accuracy: 39.02%\n",
      "Epoch [8/15], Train Loss: 1.4105, Train Accuracy: 50.91%, Val Loss: 1.7174, Val Accuracy: 39.02%\n",
      "Epoch [9/15], Train Loss: 1.3168, Train Accuracy: 54.23%, Val Loss: 1.8311, Val Accuracy: 37.17%\n",
      "New best model saved with validation accuracy: 39.62%\n",
      "Epoch [10/15], Train Loss: 1.2257, Train Accuracy: 57.09%, Val Loss: 1.7616, Val Accuracy: 39.62%\n",
      "New best model saved with validation accuracy: 40.57%\n",
      "Epoch [11/15], Train Loss: 1.1165, Train Accuracy: 62.39%, Val Loss: 1.7380, Val Accuracy: 40.57%\n",
      "New best model saved with validation accuracy: 41.02%\n",
      "Epoch [12/15], Train Loss: 1.0214, Train Accuracy: 65.25%, Val Loss: 1.7481, Val Accuracy: 41.02%\n",
      "New best model saved with validation accuracy: 42.07%\n",
      "Epoch [13/15], Train Loss: 0.8933, Train Accuracy: 70.25%, Val Loss: 1.7294, Val Accuracy: 42.07%\n",
      "New best model saved with validation accuracy: 42.42%\n",
      "Epoch [14/15], Train Loss: 0.7807, Train Accuracy: 75.30%, Val Loss: 1.7791, Val Accuracy: 42.42%\n",
      "New best model saved with validation accuracy: 42.87%\n",
      "Epoch [15/15], Train Loss: 0.6726, Train Accuracy: 78.97%, Val Loss: 1.7969, Val Accuracy: 42.87%\n",
      "Training completed. Best validation accuracy: 42.87%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.amp as amp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import wandb\n",
    "\n",
    "# Set your data paths\n",
    "data_path = data_path  # Replace with your actual path\n",
    "test_data_path = data_path_test  # Replace with your actual path\n",
    "\n",
    "# Best configuration from the sweep\n",
    "best_config = {\n",
    "    'base_filter': 64,\n",
    "    'filter_strategy': 'doubling',\n",
    "    'kernel_sizes': [3, 5, 7, 5, 3],\n",
    "    'activation': 'ReLU',\n",
    "    'batch_norm': True,\n",
    "    'dropout': 0.2,\n",
    "    'fc_size': 512,\n",
    "    'learning_rate': 3.3e-5,\n",
    "    'batch_size': 32,\n",
    "    'data_augmentation': False,\n",
    "    'epochs': 15,\n",
    "    'optimizer_type': 'Adam'\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "scaler = amp.GradScaler(enabled=device.type == 'cuda')\n",
    "\n",
    "# Define transforms based on config\n",
    "def get_transforms(data_augmentation, is_training=True):\n",
    "    \"\"\"Get the appropriate transforms based on whether we're training and using augmentation.\"\"\"\n",
    "    if is_training and data_augmentation:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Determine filter sizes based on strategy\n",
    "base_filter = best_config['base_filter']\n",
    "if best_config['filter_strategy'] == 'same':\n",
    "    filters = [base_filter] * 5\n",
    "elif best_config['filter_strategy'] == 'doubling':\n",
    "    filters = [base_filter * (2 ** i) for i in range(5)]\n",
    "elif best_config['filter_strategy'] == 'halving':\n",
    "    filters = [base_filter * (2 ** i) for i in reversed(range(5))]\n",
    "\n",
    "# Initialize model with best config\n",
    "#from cnn import SimpleCNN  # Make sure this import works\n",
    "model = SimpleCNN(\n",
    "    num_classes=10,\n",
    "    kernel_size=best_config['kernel_sizes'],\n",
    "    no_kernels=filters,\n",
    "    fc1_size=best_config['fc_size'],\n",
    "    conv_activation=best_config['activation'],\n",
    "    use_batch_norm=best_config['batch_norm'],\n",
    "    dropout=best_config['dropout']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if best_config['optimizer_type'] == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_config['learning_rate'], weight_decay=1e-4)\n",
    "elif best_config['optimizer_type'] == 'Nadam':\n",
    "    optimizer = optim.NAdam(model.parameters(), lr=best_config['learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "# Load and split dataset\n",
    "dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=get_transforms(best_config['data_augmentation'], is_training=True)\n",
    ")\n",
    "\n",
    "# Calculate split sizes (80% train, 20% validation)\n",
    "val_split = 0.2\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(val_split * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    ")\n",
    "\n",
    "# Update validation transform\n",
    "val_dataset.dataset.transform = get_transforms(False, is_training=False)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=best_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=best_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Define function to run an epoch\n",
    "def run_epoch(model, dataloader, criterion, optimizer, device, scaler, is_training=True):\n",
    "    \"\"\"Run one epoch of training or validation.\"\"\"\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass with autocast for mixed precision\n",
    "        with amp.autocast(device_type=device.type, enabled=scaler.is_enabled() and is_training):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization (only during training)\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            # Use scaler for mixed precision gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training with best configuration...\")\n",
    "best_val_accuracy = 0\n",
    "model_save_path = \"best_model.pth\"\n",
    "\n",
    "for epoch in range(best_config['epochs']):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, train_accuracy = run_epoch(model, train_loader, criterion, optimizer, device, scaler, is_training=True)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_accuracy = run_epoch(model, val_loader, criterion, optimizer, device, scaler, is_training=False)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved with validation accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{best_config['epochs']}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Training completed. Best validation accuracy: {best_val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:20:27.021828Z",
     "iopub.status.busy": "2025-04-19T06:20:27.021546Z",
     "iopub.status.idle": "2025-04-19T06:20:37.384033Z",
     "shell.execute_reply": "2025-04-19T06:20:37.383140Z",
     "shell.execute_reply.started": "2025-04-19T06:20:27.021806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1960726611.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 43.20%\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=test_data_path,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = test_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Load the best model\n",
    "model_save_path=\"/kaggle/working/best_model.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Store predictions, labels, and images for visualization\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            # Convert images to numpy for visualization (only store a subset if needed)\n",
    "            for img in images.cpu().numpy():\n",
    "                all_images.append(img)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy, all_preds, all_labels, all_images\n",
    "\n",
    "# Run the evaluation\n",
    "test_accuracy, predictions, true_labels, test_images = evaluate_model(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T06:36:03.845310Z",
     "iopub.status.busy": "2025-04-19T06:36:03.844610Z",
     "iopub.status.idle": "2025-04-19T06:36:04.311327Z",
     "shell.execute_reply": "2025-04-19T06:36:04.310693Z",
     "shell.execute_reply.started": "2025-04-19T06:36:03.845284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎨 Balanced 10×3 prediction grid saved!\n"
     ]
    }
   ],
   "source": [
    "def create_creative_prediction_grid(images, preds, labels, class_names, rows=10, cols=3):\n",
    "    import numpy as np\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "    # Normalize stats\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    num_samples = rows * cols\n",
    "\n",
    "    # Select correct/incorrect\n",
    "    correct = [i for i, (p, l) in enumerate(zip(preds, labels)) if p == l]\n",
    "    incorrect = [i for i, (p, l) in enumerate(zip(preds, labels)) if p != l]\n",
    "    n_incorrect = min(num_samples // 3, len(incorrect))\n",
    "    n_correct = num_samples - n_incorrect\n",
    "\n",
    "    selected = list(np.random.choice(correct, n_correct, replace=False)) + \\\n",
    "               list(np.random.choice(incorrect, n_incorrect, replace=False))\n",
    "    np.random.shuffle(selected)\n",
    "\n",
    "    if len(selected) < num_samples:\n",
    "        remaining = num_samples - len(selected)\n",
    "        pool = [i for i in range(len(images)) if i not in selected]\n",
    "        selected += list(np.random.choice(pool, remaining, replace=False))\n",
    "\n",
    "    # Layout settings\n",
    "    image_size = 180\n",
    "    padding_x = 40\n",
    "    padding_y = 30\n",
    "    margin = 60\n",
    "    text_height = 45\n",
    "    cell_width = image_size + padding_x\n",
    "    cell_height = image_size + text_height + padding_y\n",
    "\n",
    "    grid_width = cols * cell_width + 2 * margin\n",
    "    grid_height = rows * cell_height + 2 * margin + 80  # + title\n",
    "\n",
    "    grid_image = Image.new('RGB', (grid_width, grid_height), color=(255, 255, 255))\n",
    "    draw = ImageDraw.Draw(grid_image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"Arial.ttf\", 18)\n",
    "        small_font = ImageFont.truetype(\"Arial.ttf\", 14)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "        small_font = ImageFont.load_default()\n",
    "\n",
    "    # Title\n",
    "    title = \"Test Set Predictions (Best CNN)\"\n",
    "    title_x = (grid_width - draw.textlength(title, font=font)) // 2\n",
    "    draw.text((title_x, 20), title, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    model_info = f\"{best_config['filter_strategy'].capitalize()} filters | {best_config['activation']} | Dropout={best_config['dropout']}\"\n",
    "    draw.text((margin, 50), model_info, fill=(60, 60, 60), font=small_font)\n",
    "    draw.text((margin, 70), f\"Test Accuracy: {test_accuracy:.2f}%\", fill=(60, 60, 60), font=small_font)\n",
    "\n",
    "    # Draw each image block\n",
    "    for idx, img_idx in enumerate(selected):\n",
    "        img = images[img_idx].transpose(1, 2, 0) * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "        img_pil = Image.fromarray(img).resize((image_size, image_size), Image.BICUBIC)\n",
    "\n",
    "        row, col = divmod(idx, cols)\n",
    "        x = margin + col * cell_width\n",
    "        y = margin + row * cell_height + 80\n",
    "\n",
    "        pred = preds[img_idx]\n",
    "        label = labels[img_idx]\n",
    "        is_correct = pred == label\n",
    "\n",
    "        bg_color = (235, 255, 235) if is_correct else (255, 235, 235)\n",
    "        draw.rectangle([x - 8, y - 8, x + image_size + 8, y + image_size + text_height + 8], fill=bg_color, outline=(200, 200, 200))\n",
    "\n",
    "        grid_image.paste(img_pil, (x, y))\n",
    "\n",
    "        # Text\n",
    "        draw.text((x, y + image_size + 5), f\"True: {class_names[label]}\", fill=(0, 100, 0) if is_correct else (150, 0, 0), font=small_font)\n",
    "        draw.text((x, y + image_size + 22), f\"Pred: {class_names[pred]}\", fill=(0, 0, 100), font=small_font)\n",
    "\n",
    "    return grid_image\n",
    "grid_image = create_creative_prediction_grid(test_images, predictions, true_labels, class_names)\n",
    "grid_image.save(\"prediction_grid.png\")\n",
    "print(\"🎨 Balanced 10×3 prediction grid saved!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7125598,
     "sourceId": 11380447,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
